{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Detection_CapsuleNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammad-debug/Pneumonia-Caps/blob/main/MiniProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtWYBX2Nq5Lu"
      },
      "source": [
        "#MINOR PROJECT \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Pneumonia Detection Using Capsule Networks By Omar Dahmen\n",
        "\n",
        "Image classification has become one of the main tasks in the field of computer vision technologies. State-of-the-art approaches in automated classification use deep convolutional neural networks (CNNs). However, these networks require a large number of training samples to generalize well. This project investigates the use of capsule networks (CapsNets) as an alternative to CNNs. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-RuU8_uwX3t"
      },
      "source": [
        "# load dependencies\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf   # Using tensorflow 2.0.0\n",
        "from tensorflow.keras import layers, initializers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "IMG_SIZE = 299\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj6RwU667RlT",
        "outputId": "5c36ed45-19ae-4779-c5a6-413e21383cca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RQNt3Mo6PEs"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "def path_extractor(zipPATH,zip_file,trgt_dir):\n",
        "  # Extracting content, if \"trgt_dir\" not present\n",
        "  if 'chest_xray' not in os.listdir('.'):\n",
        "    with zipfile.ZipFile(zipPATH+zip_file,\"r\") as z:\n",
        "            print(f\"Extracting content from {zip_file} ......\")\n",
        "            z.extractall()\n",
        "            print(f\"Extracted to {os.getcwd()}\")\n",
        "  # Storing paths of images in \"trgt_dir\" to a list\n",
        "  paths = [] # Stores Image file paths\n",
        "  cls_labels = [] # Stores Class labels\n",
        "  print(f\"Reading image paths in chest_xray/{trgt_dir} directory\")\n",
        "  folder = \".\"+os.path.sep+'chest_xray'+os.path.sep+trgt_dir\n",
        "  print(f'Available classes {os.listdir(folder)}')\n",
        "  for root, dirs, files in os.walk(folder):\n",
        "      for file in files:\n",
        "        if file.endswith(\".jpeg\"):\n",
        "          f_path = os.path.join(root, file)\n",
        "          label = f_path.split('/')[-2]\n",
        "          cls_labels.append(label)\n",
        "          paths.append(f_path)\n",
        "  print(\"DONE\")\n",
        "  return paths,cls_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xitq2Dt-6PHX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqvK39bxQQ_g",
        "outputId": "5615b3e4-7b70-4988-fe5f-2994f277c937"
      },
      "source": [
        "#Downloading the dataset and unzip the train and test dataset\n",
        "# Location of zipfile containing dataset\n",
        "PATH = \"/content/drive/MyDrive/\"\n",
        "zip_file = \"archive.zip\"\n",
        "\n",
        "trainImg_paths, y_train = path_extractor(PATH,zip_file,trgt_dir=\"train\")\n",
        "testImg_paths, y_test = path_extractor(PATH,zip_file,trgt_dir=\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting content from archive.zip ......\n",
            "Extracted to /content\n",
            "Reading image paths in chest_xray/train directory\n",
            "Available classes ['NORMAL', '.DS_Store', 'PNEUMONIA']\n",
            "DONE\n",
            "Reading image paths in chest_xray/test directory\n",
            "Available classes ['NORMAL', '.DS_Store', 'PNEUMONIA']\n",
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Mn2FzuQgVD"
      },
      "source": [
        "###Image Augmentation\n",
        "Image Augmentation is a very simple, but very powerful tool to help you avoid overfitting your data.\n",
        "We have used keras image preprossessing tool. It doesn't require us to edit our raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you to experiment without impacting our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgPFoCx3wrY3"
      },
      "source": [
        "# This code block should create an instance of an ImageDataGenerator called datagen \n",
        "def DataGenerator(train_batch, val_batch, IMG_SIZE):\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                 rescale=1./255,#rescale factor\n",
        "                                 rotation_range=10,# Degree range for random rotations.\n",
        "                                 horizontal_flip=True,#horizontal flip in images\n",
        "                                 vertical_flip=True)#vertical flip in images\n",
        "\n",
        "    datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "\n",
        "    train_gen = datagen.flow_from_directory('/content/chest_xray/train/',\n",
        "                                            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                            color_mode='rgb', \n",
        "                                            class_mode='categorical',\n",
        "                                            batch_size=train_batch)\n",
        "\n",
        "    val_gen = datagen.flow_from_directory('/content/chest_xray/val/', \n",
        "                                          target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                          color_mode='rgb', \n",
        "                                          class_mode='categorical',\n",
        "                                          batch_size=val_batch)\n",
        "\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                 rescale=1./255)\n",
        "    \n",
        "    datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "\n",
        "    test_gen = datagen.flow_from_directory('/content/chest_xray/test/', \n",
        "                                           target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                           color_mode='rgb', \n",
        "                                           class_mode='categorical',\n",
        "                                           shuffle=False)\n",
        "    \n",
        "    return train_gen, val_gen, test_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wr5tZhSBte"
      },
      "source": [
        "### Some setup functions for our Model\n",
        "\n",
        "- Squash function is used to normalize the magnitude of vectors, rather than the scalar elements themselves. The outputs from these squash functions tell us how to route data through various capsules that are trained to learn different concepts. \n",
        "- The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution\n",
        "- Margin loss function representing the price paid for inaccuracy of predictions in our classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2G8NU2wv8b"
      },
      "source": [
        "# the squashing function.\n",
        "\"\"\"\n",
        "The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        ":param vectors: some vectors to be squashed, N-dim tensor\n",
        ":param axis: the axis to squash\n",
        ":return: a Tensor with same shape as input vectors\n",
        "\"\"\"\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
        "    return scale * x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWvGDTicw3lM"
      },
      "source": [
        "# define our own softmax function instead of K.softmax\n",
        "# because K.softmax can not specify axis.\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex / K.sum(ex, axis=axis, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVz7WaGgw8dz"
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1 #default lambda 0.5 - but test with lambda with 0.9 - 0.1\n",
        "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzbzI5AXVU3s"
      },
      "source": [
        "##Class setup for Capsule Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6lMHPhxIn7"
      },
      "source": [
        "def caps_batch_dot(x, y):\n",
        "    x = K.expand_dims(x, 2)\n",
        "    if K.int_shape(x)[3] is not None:\n",
        "        y = K.permute_dimensions(y, (0, 1, 3, 2))\n",
        "    o = tf.matmul(x, y)\n",
        "    return K.squeeze(o, 2)\n",
        "\n",
        "class Capsule(Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 share_weights=True,\n",
        "                 activation='squash',\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(1, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "        else:\n",
        "            if input_shape[-2] is None:\n",
        "                raise ValueError(\"Input Shape must be defied if weights not shared.\")\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(input_num_capsule, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
        "        but replace b = b + <u,v> with b = <u,v>.\n",
        "        This change can improve the feature representation of Capsule.\n",
        "        However, you can replace\n",
        "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        with\n",
        "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        to realize a standard routing.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule,\n",
        "                                self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "\n",
        "        \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = self.activation(caps_batch_dot(c, hat_inputs))\n",
        "            if i < self.routings - 1:\n",
        "                b = caps_batch_dot(o, hat_inputs)\n",
        "                if K.backend() == 'theano':\n",
        "                    o = K.sum(o, axis=1)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(Capsule, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0nVTBF-xRJr",
        "outputId": "2abdf6ce-f9f6-45e4-bf84-bb8faea15224"
      },
      "source": [
        "#Generating data for training using Image Generator defined above\n",
        "train_batch = 32\n",
        "val_batch = 1\n",
        "\n",
        "train, val, test = DataGenerator(train_batch, val_batch, IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHvO9_P-mbv3"
      },
      "source": [
        "### Importing  VGG16 as our base model (Transfer Learning)\n",
        "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n",
        "\n",
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5V4pY9p_f2i",
        "outputId": "bc4d261d-c5c2-49d9-aa8a-082bb509246d"
      },
      "source": [
        "input_image = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "\n",
        "# A InceptionResNetV2 Conv2D model\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_tensor=input_image)\n",
        "\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXOY8-Bn-UP"
      },
      "source": [
        "##Network I\n",
        "In our first network, our first layer (base layer) is VGG 16 with its layers frozen. After that, we have applied a Global average pooling layer. Then we have a dense layer of 4096 neurons with activation \"Relu\". after that we have a dropout layer with rate 0.5 and finally an output \"softmax\" layer with 2 outcomes. The loss function we used in our network was categorical_crossentropy and optimizer was RMSprop with learning rate=1e-4. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyTea6VLsKz4"
      },
      "source": [
        "####Feature extraction\n",
        " We will freeze the convolutional base created from the previous step and to use as a feature extractor.\n",
        "\n",
        "####Freeze the convolutional base\n",
        "It is important to freeze the convolutional base before we compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. VGG 16 has many layers, so setting the entire model's trainable flag to False will freeze all of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PBjEN0n_rna",
        "outputId": "5ff1276d-8620-44e4-ccdf-58fa0d7263ba"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7f8b13979710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13972e50> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13ba0f50> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f8b13d8d390> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d965d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d9e210> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f8b13d96dd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13da6050> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13daa9d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d33bd0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f8b13da6910> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d38690> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d3f290> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d38950> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f8b13d47d10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d4f490> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d55c90> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7f8b13d47850> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7f8b13d5d5d0> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAYIXJRD_1Q-",
        "outputId": "d945f8df-394e-494e-add4-4050ecca8fdd"
      },
      "source": [
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 16,824,130\n",
            "Trainable params: 2,109,442\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVAzZMveraR9"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "  optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "  metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjlaAqBEItDf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8152pVGJAMj8",
        "outputId": "31c999f7-639f-4729-b71c-6e9d5cb29b95"
      },
      "source": [
        "model.fit(train,\n",
        "                    epochs=1,\n",
        "                    validation_data=val, \n",
        "                    validation_steps = len(val.classes)//val_batch,\n",
        "                    steps_per_epoch = len(train.classes)//train_batch)  \n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 4163s 26s/step - loss: 0.4303 - acc: 0.7853 - val_loss: 0.6662 - val_acc: 0.6250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2006: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n",
            "Loss: 0.4756128489971161\n",
            "Accuracy: 73.24 %\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99e4v21F9kWs"
      },
      "source": [
        "##Network II\n",
        "In our second network, the architecture is almost the same. We have fine tuned our VGG_16 net and the loss function we used here was categorical_crossentropy and optimizer was SGD with learning rate=1e-4 and momentum=0.9. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtPteq6e-Qjf"
      },
      "source": [
        "###Fine Tuning\n",
        "In the feature extraction, we were only training a few layers on top of an VGG_16 base model. The weights of the pre-trained network were not updated during training.\n",
        "\n",
        "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3mqbu2exryW"
      },
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    if i < 15:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSymiSo1xwaU"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=1e-4, momentum=0.9), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAq_RPuox0p-",
        "outputId": "e295050f-eeba-4192-fe0d-fb959356920e"
      },
      "source": [
        "history=model.fit(train,\n",
        "          epochs=10,\n",
        "          validation_data=val, \n",
        "          validation_steps = len(val.classes)//val_batch,\n",
        "          #steps_per_epoch=(len(train.classes)//train_batch) * 2\n",
        "          ) \n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "163/163 [==============================] - 5095s 31s/step - loss: 0.2384 - accuracy: 0.8972 - val_loss: 1.1267 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "163/163 [==============================] - 5064s 31s/step - loss: 0.1770 - accuracy: 0.9304 - val_loss: 0.8297 - val_accuracy: 0.6250\n",
            "Epoch 3/10\n",
            " 20/163 [==>...........................] - ETA: 1:13:12 - loss: 0.1716 - accuracy: 0.9312"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vM3xINnfUKNO"
      },
      "source": [
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We6nJC1__Q_h"
      },
      "source": [
        "##Network III\n",
        "In our third network, our first layer (base layer) is VGG 16. After that, we have a convolutional 2D layer of 256 filters with stride=1,9x9 kernel size and activation function=\"Relu\". Now we finally have our Capsule Network comprising followed by lambda ouput\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JsG3nLpZx5ei"
      },
      "source": [
        "output = Conv2D(256, kernel_size=(9, 9), strides=(1, 1), activation='relu')(base_model.get_layer(name='block5_pool').output)\n",
        "\n",
        "x = Reshape((-1, 256))(output)\n",
        "capsule = Capsule(2, 16, 4, True)(x)\n",
        "output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YkJ_Pyjpx_Kn"
      },
      "source": [
        "lr=1e-4\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"weights.h5\", \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=False, \n",
        "                             mode='min')\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)\n",
        "\n",
        "callback_list = [checkpoint, early]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nprkIHBPuz86"
      },
      "source": [
        "epochs=20\n",
        "\n",
        "model.compile(loss=margin_loss, optimizer=SGD(lr=lr, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history=model.fit(train,\n",
        "          epochs=epochs,\n",
        "          validation_data=val, \n",
        "          validation_steps = len(val.classes)//val_batch,\n",
        "          steps_per_epoch=len(train.classes)//train_batch,\n",
        "          callbacks=callback_list\n",
        "          )\n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xkmcnXz_cPF"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovcXJ-Zg_cTD"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}